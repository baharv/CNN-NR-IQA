# -*- coding: utf-8 -*-
"""last of CNNIQA-modified.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tu9UeUSWpZyIQtNxJHZkaKxqe20KoUlH
"""


import shutil
from tensorflow import keras
import tensorflow as tf
import numpy as np
shutil.unpack_archive('database.zip','.')

import scipy.io
mat = scipy.io.loadmat('dmos.mat')

mat['orgs'].shape

# Define Model
def model1():

  from tensorflow import keras
  import tensorflow as tf
  import numpy as np
  keras.backend.clear_session()
  shape=(32,32,1)
  input_tensor=keras.Input(shape=shape)
  conv1=keras.layers.Conv2D(50,(7,7),padding='valid',strides=1)(input_tensor)
  mxpool=keras.layers.GlobalMaxPooling2D()(conv1)
  mnpool=-1*keras.layers.GlobalMaxPooling2D()(-1*conv1)
  concat=keras.layers.Concatenate(axis=1)([mxpool,mnpool])
  dense1=keras.layers.Dense(800,activation='relu')(concat)
  dp=keras.layers.Dropout(0.5)(dense1)
  dense2=keras.layers.Dense(800,activation='relu')(dp)
  out=keras.layers.Dense(1)(dense2)
  model=keras.Model(inputs=input_tensor,outputs=out)
  model.summary()
  return(model)

#Generator2
import numpy as np
import csv
import os
import tensorflow as tf
from PIL import Image
from torchvision.transforms.functional import to_tensor
import torch
from scipy.signal import convolve2d
import random
def LocalNormalization(patch, P=3, Q=3, C=1):
    kernel = np.ones((P, Q)) / ((P * Q))
    patch_mean = convolve2d(patch, kernel, boundary='symm', mode='same')
    patch_sm = convolve2d(np.square(patch), kernel, boundary='symm', mode='same')
    patch_std = np.sqrt(np.maximum(patch_sm - np.square(patch_mean), 0)) + C
    patch_ln = torch.from_numpy((patch - patch_mean) / patch_std).float().unsqueeze(0)
    return patch_ln

def NonOverlappingCropPatches(im, patch_size=32, stride=32):
    w, h = im.size
    patches = ()
    for i in range(0, h - stride, stride):
        for j in range(0, w - stride, stride):
            patch = to_tensor(im.crop((j, i, j + patch_size, i + patch_size)))
            patch = LocalNormalization(patch[0].numpy())
            patch=patch.numpy()
            patch=patch.reshape((patch.shape[1],patch.shape[2],1))
            #print(patch.shape)
            patches = patches + (patch,)
    return patches
    
class DataGenerator2(tf.keras.utils.Sequence):
    def __init__(self, csv_dir, img_dir, batch_size=32, shuffle=True,patch_size=32,stride=32):
        self.batch_size = batch_size
        self.csv_dir=csv_dir
        self.img_dir=img_dir
        self.shuffle = shuffle
        self.data=[]
        self.patch_size=patch_size#32
        self.stride=stride#32
        with open(csv_dir, newline='',mode='r') as csvfile:#csv test va train ro mikhone  va save mikone
            csvreader=csv.reader(csvfile)
            for row in csvreader:
              #if 'jp2k' in row[0]:
              self.data.append(row)

        self.indices = len(self.data)#tedat dade ha
        self.on_epoch_end()
        random.shuffle(self.data)#suffle mikone ta ravand amadan dadeha random shavad
    def __len__(self):
        """baray inke 2 ta akahr ezaf
        if self.indices % self.batch_size !=0:
          if self.indices // self.batch_size ==0:
            return (self.indices // self.batch_size)
          return (self.indices // self.batch_size)+1
        """
        return (self.indices // self.batch_size)#tedad gam hai ke baray train mire ra teen mikond

    def __getitem__(self, index):#axha ra mikhone az addressi ke dare
        index = self.index[index * self.batch_size:(index + 1) * self.batch_size]#ex agar batch_size=6 bashe va index=0, mishavad az 0 ta 6
        #yani 6 ta ax aval ra bekhon
        batch = [k for k in index]    #k for k===> be eza on chizhai ke to index hast yek list dorost mikonad    
        X, y = self.__get_data(batch) #axha ro mikhone az batch
        return X, y

    def on_epoch_end(self):
        self.index = np.arange(self.indices)
        if self.shuffle == True:
            np.random.shuffle(self.index)

    def __get_data(self, batch):
        scores=[]
        images=np.zeros((len(batch),32,32,1),dtype=np.float32)#miad yek matis khali be sorat len batch,32,32,1
        for i, id in enumerate(batch): 
            img=cv2.imread(self.data[i][0],cv2.IMREAD_UNCHANGED)#done done ax haro mikhone
            images[i,:,:,:]=np.expand_dims(img,axis=2)#ezafe mikone va dar images zakhire mikonad /expand_dims=ax 2 boodi ra be 3 boadi tabdil
            scores.append(float(self.data[i][1]))
        scores=np.array(scores)#score ra niz dar list score zakhire karde
        scores=scores.reshape((len(scores),1))
        return images, scores

import cv2
import random

def process(mode='wn'):
  # Create dataset
  import csv
  img_names=[]
  scores=[]
  refs=[]

  img_data={}
  if mode=='jp2k':
    extra_num=0
  elif mode=='jpeg':
    extra_num=227
  elif mode=='wn':
    extra_num=460
  elif mode=='gblur':
    extra_num=634
  elif mode=='fastfading':
    extra_num=808
    
  with open('{}/info.txt'.format(mode),'r') as f:#file txt mikhone
    lines=f.readlines()
  for line in lines:
    line=line.split()
    if len(line)!=0:
      #if float(line[2])!=0:
        img_data[line[1]]=line[0]
        #img_names.append(line[1])
        #scores.append(line[2])
        #refs.append(line[0])
  for imn in range(1,len(img_data.keys())+1):
    if mat['orgs'][0,imn+extra_num-1]==1:
      continue
    img_names.append('img{}.bmp'.format(imn))
    scores.append(mat['dmos'][0,imn+extra_num-1])
    refs.append(img_data['img{}.bmp'.format(imn)])

  unique_refs=(list(np.unique(refs).copy()))
  random.shuffle(unique_refs) #enable shuffling
  test_refs=unique_refs[:len(unique_refs)//5]
  train_refs=unique_refs[len(unique_refs)//5:]
  #print(test_refs)
  #print(train_refs)
  with open('{}-test-live.csv'.format(mode),'w',newline='') as csvfile:
    csvwriter=csv.writer(csvfile,delimiter=',')
    for row in img_names:
      if refs[img_names.index(row)] in test_refs:
        csvwriter.writerow(['{}/{}'.format(mode,row),scores[img_names.index(row)],refs[img_names.index(row)]])
  with open('{}-train-live.csv'.format(mode),'w',newline='') as csvfile:
    csvwriter=csv.writer(csvfile,delimiter=',')
    for row in img_names:
      if refs[img_names.index(row)] in train_refs:
        csvwriter.writerow(['{}/{}'.format(mode,row),scores[img_names.index(row)],refs[img_names.index(row)]])
  ####################
  # Create patches from the dataset
  import cv2
  #try:
  #  shutil.rmtree('patches')
  #except:
  #  pass
  #!mkdir 'patches'
  test_data=[]
  train_data=[]
  new_test=[]
  new_train=[]
  with open('{}-test-live.csv'.format(mode),'r',newline='') as csvfile:
    csvr=csv.reader(csvfile,delimiter=',')
    for row in csvr:
      test_data.append(row)
  with open('{}-train-live.csv'.format(mode),'r',newline='') as csvfile:
    csvr=csv.reader(csvfile,delimiter=',')
    for row in csvr:
      train_data.append(row)
  for da in test_data:
    indx=da[0].index('img')
    try:
      os.mkdir('patches/{}'.format(da[0][:indx]))
    except:
      pass
    img=Image.open(da[0]).convert('L')
    patches = NonOverlappingCropPatches(img, 32,32)
    patches=np.array(patches)
    for pa in range(patches.shape[0]):
      cv2.imwrite('patches/{}-{}.tif'.format(da[0][:-4],pa),patches[pa,:,:,:])
      new_test.append(['patches/{}-{}.tif'.format(da[0][:-4],pa),float(da[1])])
  for da in train_data:
    indx=da[0].index('img')
    try:
      os.mkdir('patches/{}'.format(da[0][:indx]))
    except:
      pass
    img=Image.open(da[0]).convert('L')
    patches = NonOverlappingCropPatches(img, 32,32)
    patches=np.array(patches)
    for pa in range(patches.shape[0]):
      cv2.imwrite('patches/{}-{}.tif'.format(da[0][:-4],pa),patches[pa,:,:,:])
      new_train.append(['patches/{}-{}.tif'.format(da[0][:-4],pa),float(da[1])])

  with open('test-live-new.csv','w',newline='') as csvfile:
    csvwriter=csv.writer(csvfile,delimiter=',')
    for row in new_test:
      csvwriter.writerow(row)
  with open('train-live-new.csv','w',newline='') as csvfile:
    csvwriter=csv.writer(csvfile,delimiter=',')
    for row in new_train:
      csvwriter.writerow(row)
  #########################
  # Create generator2
  train_generator2=DataGenerator2(csv_dir='train-live-new.csv',img_dir='.',batch_size=256)
  test_generator2=DataGenerator2(csv_dir='test-live-new.csv',img_dir='.',batch_size=10)
  # Checkpoints and Callbacks
  !mkdir 'models'
  filepath="models/%s-{epoch:02d}-{val_loss:.4f}.hdf5"%mode # Path to save the trained models
  checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True) #creating checkpoint to save the best validation accuracy
  callbacks_list = [checkpoint]
  model2=model1()
  model2.compile(optimizer=keras.optimizers.Adam(lr=0.001), loss=keras.losses.MeanAbsoluteError())
  #Train model based on generator2
  model2.fit_generator(train_generator2, epochs=10,shuffle=True,validation_data=test_generator2,callbacks=callbacks_list)
  ##############################
  # Report the results for generator2
  from scipy import stats
  gt_scores=[]
  pred_scores=[]
  differ=[]
  images=np.zeros((test_generator2.indices,32,32,1),dtype=np.float32)
  for index in range(test_generator2.indices):
    img=cv2.imread(test_generator2.data[index][0],cv2.IMREAD_UNCHANGED)
    images[index,:,:,:]=np.expand_dims(img,axis=2)
    gt_scores.append(float(test_generator2.data[index][1]))
  indd=0
  while(True):
    try:
      results=model2.predict(images[indd*1000:(indd+1)*1000,:,:,:])
      indd+=1
      pred_scores.extend(results)
    except:
      break
  pred_scores=np.reshape(np.array(pred_scores), (-1,))
  gt_scores=np.reshape(np.array(gt_scores), (-1,))
  differ=abs(np.array(gt_scores)-np.array(pred_scores))
  print(mode)
  print('MAE: ', differ.mean())
  print('srocc: ',stats.spearmanr(gt_scores, pred_scores)[0])
  print('krocc: ',stats.stats.kendalltau(gt_scores, pred_scores)[0])
  print('plcc: ',stats.pearsonr(gt_scores, pred_scores)[0])
  print('rmse: ',np.sqrt(((gt_scores - pred_scores) ** 2).mean()))
  print('############################################################')

#@title Default title text
modes=['jp2k', 'jpeg', 'wn', 'gblur', 'fastfading']
for mode in modes:
  process(mode)


